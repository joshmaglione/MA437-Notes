\documentclass[a4paper, 12pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,
	urlcolor=blue,
	citecolor=blue,
}
\usepackage[margin=3cm]{geometry}
\usepackage{mathpazo}
\usepackage{url}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{mathrsfs}
\usepackage{afterpage}
\usepackage[svgnames]{xcolor}
\usepackage{marvosym}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{pgfplots}
\usetikzlibrary{3d}

\numberwithin{equation}{section}
\numberwithin{figure}{section}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem*{con*}{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{quest}[thm]{Question}
\newtheorem{obs}[thm]{Observation}
\newtheorem{notation}[thm]{Notation}
\newtheorem{exercise}{Exercise}

\newenvironment{mybox}[1][]{%
\ifstrempty{#1}%
% if condition (without title)
{\mdfsetup{%
    frametitle={%
        %\tikz[baseline=(current bounding box.east),outer sep=0pt]
        %\node[anchor=east,rectangle,fill=RoyalBlue!80] {};
		}
    }%
% else condition (with title)
}{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=RoyalBlue!80,text=white]
        {\strut #1};}%
    }%
}%
% Both conditions
\mdfsetup{%
    innertopmargin=10pt,linecolor=RoyalBlue!80,%
    linewidth=2pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax%
}
\begin{mdframed}[]\relax}{%
\end{mdframed}}

\newenvironment{myboxgreen}[1][]{%
\ifstrempty{#1}%
% if condition (without title)
{\mdfsetup{%
    frametitle={%
        %\tikz[baseline=(current bounding box.east),outer sep=0pt]
        %\node[anchor=east,rectangle,fill=RoyalBlue!80] {};
		}
    }%
% else condition (with title)
}{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=ForestGreen!80,text=white]
        {\strut #1};}%
    }%
}%
% Both conditions
\mdfsetup{%
    innertopmargin=10pt,linecolor=ForestGreen!80,%
    linewidth=2pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax%
}
\begin{mdframed}[]\relax}{%
\end{mdframed}}

\newenvironment{myboxex}[1][1]{%
\vspace{0.25em}
\noindent \begin{minipage}{0.99\textwidth}
	\centering
	\begin{minipage}{#1\textwidth}
		\begin{myboxgreen}[]{}
}{%
		\end{myboxgreen}
	\end{minipage}
\end{minipage}
\vspace{0.25em}
}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\define}[1]{\textbf{\textit{#1}}}
\newcommand{\WEEK}[1]{%
\hfill Week #1

\vspace{-1em}

\begin{center}
	\rule{\textwidth}{2pt}
\end{center}
\vspace{0.5em}%
}

\setcounter{tocdepth}{2}

\allowdisplaybreaks

\title{The Mathematics of Decision Making I}
\author{Joshua Maglione}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}

The mathematics of decision making is very closely tied to the field of
mathematical optimization. One of the primary ways mathematics is used to help
guide decisions is by maximizing (or minimizing) specific outcomes subject to a
list of constraints. Mathematical optimization provides the formal tools to
model and solve such problems.

There are many kinds of mathematical optimization. There are two basic types
depending on whether the variables to optimize or discrete or continuous. A few
types of optimization are\footnote{``Program'' is not a computer program but
comes from the United States military's use of the word for training and
logistics schedules.}
\begin{itemize}
	\item Linear Programming,
	\item Integer Programming,
	\item Stochastic programming,
	\item Combinatorial optimization,
	\item Dynamic programming.
\end{itemize}
Unsurprisingly there are many real-world applications; to list a few we have
network optimization, pricing strategy, scheduling, supervised machine learning
training, supply chain optimization, and transportation problems. 

In this module, we will introduce the fundamentals of \textbf{linear
programming}, also called \emph{linear optimization} and \emph{operations
research}, such as the simplex method, polyhedral geometry, and the notion of
duality. Depending on the time, we may also delve into \textbf{integer
programming}.

\subsection{History}

Mathematical optimization has quite an interesting history. In the 17th century,
combinatorial optimization problems were solved using game theory,
combinatorics, and ad hoc methods. In the 19th century, transportation problems
involving post and rail were studied and solved. And in the 20th century with
the two World Wars and rise of the assembly line, operations research took off
developing the mathematics for all kinds of optimization problems. 

One of the most influential figures in mathematical optimization, and linear
programming in particular, is George Dantzig. He was the recipient of the
President's National Medal of Science in 1975 \cite{NSF} and was credited for 
\begin{quote}
	\emph{inventing linear programming and discovering methods that led to wide-scale scientific and technical applications to important problems in logistics, scheduling, and network optimization, and to the use of computers in making efficient use of the mathematical theory.}
\end{quote}
The proof of the simplex method, name coined by Motskin, was developed by
Dantzig in the late 1940s~\cite{Dantzig}. I find it interesting that the
``inductive proof of the simplex method'' was published by the Mathematics
Division of the RAND Corporation in 1960 (by Dantzig) and was made
classified~\cite{SimplexMethod}. Now, of course, it is no longer classified.

After explaining the Simplex Method to John von Neumann at the Institute of
Advanced Study in Princeton during 1948, von Neumann immediately conjectured the
notion of duality because of his recent foray into game theory. 

\subsection{Four examples}

We describe four example problems that touch on the tools we will develop in
this module. For now, these problems are meant to introduce basic concepts and
vocabulary.

\subsubsection{A diet problem}

Erin is planning her breakfast and wants to make oats with milk. (These
numbers of simplified and not accurate to real life.)
\begin{center}
	\begin{tabular}{|r|c|c|} \hline
		& Milk (100ml) & Oats (100g) \\ \hline 
		fat & 2g & 3g \\ 
		carbohydrates & 1g & 3g \\
		protein & 4g & 3g \\ \hline
	\end{tabular}
\end{center}

Erin wants the meal to provide at least 18g of fat, at least 12g of
carbohydrates, and at least 24g of protein. If milk costs 20 cents per 100ml and
oats 25 cents per 100g, what mixture minimizes the cost of the desired meal?

We could express this more mathematically. For example, let $x$ and $y$ be
variables such that $x = 1$ means 100ml of milk and $y=1$ means 100g of oats.
Calculating the grams of fat relative to $x$ and $y$ is 
\[
	2x + 3y.
\]
For carbohydrates it is $x + 3y$, and for protein it is $4x+3y$. Because we want
\emph{at least} 18g of fat, we express this via 
\[ 
	2x + 3y \geq 18.
\] 
We can set up similar inequalities for the other two:
\begin{align*}
	2x + 3y &\geq 18, \\
	x + 3y &\geq 12, \\
	4x + 3y &\geq 24.
\end{align*}
Since we cannot have negative amounts of milk or oats, we have $x\geq 0$ and
$y\geq 0$. Since we want to minimize costs, we want to minimize 
\begin{align*}
	C &= 0.2 x + 0.25 y.
\end{align*}
Putting all of this together, we have the following optimization problem.

\begin{myboxex}[0.75]
	Determine values for $x$ and $y$ that minimize 
	\begin{align*}
		C &= 0.2 x + 0.25 y
	\end{align*}
	subject to the constraints: $x\geq 0$, $y\geq 0$, and 
	\begin{align*}
		2x + 3y &\geq 18, \\
		x + 3y &\geq 12, \\
		4x + 3y &\geq 24.
	\end{align*}
\end{myboxex}


\subsubsection{A transportation problem}

Javier has two production sites: one in Sligo and another in Kilkenny. There are
three distributing warehouses in Dublin, Galway, and Cork. The Sligo site can
supply 120 products per week, whereas the site in Kilkenny can supply 140 per
week. The warehouses in Dublin, Galway, and Cork need 100, 60, and 80 products
per week respectively to meet demand. The shipping costs are giving in the
following table.

\begin{center}
	\begin{tabular}{|c|ccc|} \hline 
		& Dublin & Galway & Cork \\ \hline 
		Sligo & 5 & 7 & 9 \\ 
		Kilkenny & 6 & 7 & 10 \\ \hline
	\end{tabular}
\end{center}

How many products should Javier ship from each production site to minimize total
shipping costs while still meeting demand?

We need many variables, so let's define a variable for each shipment---for example, from Kilkenny to Dublin. Write them as 
\[
	x_{kd}, x_{kg}, x_{kc}, x_{sd}, x_{sg}, x_{sc}. 
\]
Since Kilkenny and Sligo can only produce 140 and 120 products, respectively, we have
\begin{align*}
	x_{kd} + x_{kg} + x_{kc} &\leq 140, \\
	x_{sd} + x_{sg} + x_{sc} &\leq 120.
\end{align*}
We need to meet demands, so we have 
\begin{align*}
	x_{kd} + x_{sd} &\geq 100, \\
	x_{kg} + x_{sg} &\geq 60, \\
	x_{kc} + x_{sc} &\geq 80. 
\end{align*}
Lastly, we want to minimize cost, so we want to minimize
\begin{align*}
	C &= 6x_{kd} + 7x_{kg} + 10x_{kc} + 5x_{sd} + 7x_{sg} + 9x_{sc}.
\end{align*}
Altogether we have the following linear program.

\begin{myboxex}[0.75]
	Minimize 
	\begin{align*}
		C &= 6x_{kd} + 7x_{kg} + 10x_{kc} + 5x_{sd} + 7x_{sg} + 9x_{sc}
	\end{align*}
	subject to the constraints: $x_{ij}\geq 0$ for all $i$ and $j$ and 
	\begin{align*}
		x_{kd} + x_{kg} + x_{kc} &\leq 140, \\
		x_{sd} + x_{sg} + x_{sc} &\leq 120, \\
		x_{kd} + x_{sd} &\geq 100, \\
		x_{kg} + x_{sg} &\geq 60, \\
		x_{kc} + x_{sc} &\geq 80.  
	\end{align*}
\end{myboxex}

\subsubsection{The travelling salesperson problem}

Kofi need to deliver $n$ products in $n$ different cities starting in Paris. He
wants to do this by visiting each city exactly one time and then returning back
to Paris at the end. Which path minimizes the distance traveled? 

This problem is perhaps the most famous combinatorial optimization problem and
is the core problem of many other more complex problems. We will not do much
more with this, but note that different ``distance functions'' can allow for all
kinds of slow-downs and speed-ups.

\subsubsection{A financial problem}

Julia runs an investment and must invest exactly \EUR 100,000 in two types of
securities: bond \textsf{A} paying a dividend of 7\% and stock \textsf{B} paying
a dividend of 9\%. Due to her incredible experience, she knows that 
\begin{itemize}
	\item no more than \EUR 40,000 can be invested in stock \textsf{B} and
	\item the amount invested in bond \textsf{A} must be at least twice that in
	stock \textsf{B}.
\end{itemize}
How much should Julia invest in each security to maximize her return?

See if you can get the following set up.

\begin{myboxex}[0.75]
	Maximize
	\begin{align*}
		z &= 0.07A + 0.09B
	\end{align*}
	subject to the constraints: $A\geq 0$, $B\geq 0$, and 
	\begin{align*}
		A + B &= 100000, \\
		B &\leq 40000, \\
		A &\geq 2B.
	\end{align*}
\end{myboxex}


\section{General linear programming}

Linear programs are the basis of what we consider throughout this module. In the
example problems above, we sometimes wanted to maximize and sometimes we wanted
to minimize. Although these are technically different, we can treat them as the
same. Suppose $f$ is some function we want to maximize. Then 
\[ 
	\max(f) = -\min(-f).
\] 
So maximizing $f$ is the same as minimizing $-f$. Thus, we can use the two
interchangeably---as long as we correctly compensate!

\begin{mybox}[General linear program]
	Determine values for $x_1,x_2,\dots, x_n$ that maximize 
	\[
		z = c_1 x_1 + c_2 x_2 + \cdots c_n x_n 
	\]
	subject to the constraints:
	\begin{align*}
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n ~&\square~ b_1, \\
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n ~&\square~ b_2, \\
		\vdots \qquad\quad \vdots \qquad\quad \vdots \qquad\quad \vdots \quad ~&\square~ \; \vdots \\
		a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n ~&\square~ b_m,
	\end{align*}
	where each of the $\square$ can be replaced with one of $\{=,\leq,\geq\}$.
\end{mybox}

\begin{defn}
	A \define{linear program (LP) problem} is a problem of the form above. The
	function $z$ is called the \define{objective function}, and the $m$
	(in-)equalities are called the \define{constraints}.
\end{defn}

A key feature of LPs is that the objective function as well as each of the
constraint (in-)equalities are \emph{linear} in the $x_1,x_2,\dots, x_n$.

\subsection{Standard form}

Can we play around with the constants $a_{ij}$ and $b_k$ to get all of the
(in-)equalities into the same ``shape''? For example, 
\[ 
	4x_1 - 5x_2 - x_3 \geq 1
\] 
is equivalent to 
\[ 
	-4x_1 + 5x_2 + x_3 \leq -1.
\] 
Thus, if we have an inequality, we can force it to use just $\leq$. Moreover, if
we have an equality, we can use two inequalities to obtain the same solutions:
\[
	4x_1 - 5x_2 - x_3 = 1 \qquad \text{is equivalent to} \qquad \begin{cases}
		4x_1 - 5x_2 - x_3 \geq 1 ~\text{and} \\
		4x_1 - 5x_2 - x_3 \leq 1
	\end{cases}
\]
So we can transform equalities to inequalities, but what about the other way
around? We will look at this soon.

In some examples, variables only took on non-negative values. This actually has
an advantage of constraining the possible values of the variables, and it is
something we will come back to later on. But what about situations were
variables are allowed to have negative values? Suppose $x_i$ can be negative. We
can introduce two new variables, say, $x_i^+$ and $x_i^-$, and we can rewrite
$x_i$ as follows:
\[ 
	x_i = x_i^+  - x_i^-.
\] 
In this way, $x_i$ can be negative while both $x_i^+$ and $x_i^-$ are
non-negative. Thus, we can replace all instances of $x_i$ with $x_i^+  - x_i^-$,
so that all variables take non-negative values. 

Now we can define the standard form for an LP. 

\begin{mybox}[Linear program standard form]
	Determine values for $x_1,x_2,\dots, x_n$ that maximize 
	\[
		z = c_1 x_1 + c_2 x_2 + \cdots c_n x_n 
	\]
	subject to the constraints: for all $i \in \{1,\dots, n\}$, $x_i\geq 0$ and
	\begin{align*}
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &\leq b_1, \\
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &\leq b_2, \\
		\vdots \qquad\quad \vdots \qquad\quad \vdots \qquad\quad \vdots \quad ~&\quad~ \; \vdots \\
		a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &\leq b_m.
	\end{align*}
\end{mybox}

\begin{ex}
	The following LP is not in standard form.

	\begin{myboxex}[0.65]
		Determine values for $x$ and $y$ that minimize 
		\[ 
			z = 3x + 2y
		\]
		subject to the constraints: $x\geq 0$, $y\geq 0$, and 
		\begin{align*}
			2x + y &\leq 4 \\ 
			3x - 2y &\leq 6. 
		\end{align*}
	\end{myboxex}

	\noindent We can put it into standard form as follows.

	\begin{myboxex}[0.65]
		Determine values for $x$ and $y$ that maximize 
		\[ 
			z = -3x - 2y
		\]
		subject to the constraints: $x\geq 0$, $y\geq 0$, and 
		\begin{align*}
			2x + y &\leq 4 \\ 
			3x - 2y &\leq 6. 
		\end{align*}
	\end{myboxex}
\end{ex}

\begin{ex}
	Put the following LP into standard form.

	\begin{myboxex}[0.65]
		Determine values for $x$ and $y$ that minimize 
		\begin{align*}
			z &= -4x + y
		\end{align*}
		subject to the constraints:
		\begin{align*}
			x - 3y &= 2, \\ 
			x + y &\leq 6.
		\end{align*}
	\end{myboxex}
\end{ex}

\subsection{Canonical form}

The canonical form is slightly different to that of the standard form of an LP.

\begin{mybox}[Linear program canonical form]
	Determine values for $x_1,x_2,\dots, x_s$ that maximize 
	\[
		z = c_1 x_1 + c_2 x_2 + \cdots c_s x_s 
	\]
	subject to the constraints: for all $i \in \{1,\dots, s\}$, $x_i\geq 0$ and
	\begin{align*}
		a_{11}x_1 + a_{12}x_2 + \cdots + a_{1s}x_s &= b_1, \\
		a_{21}x_1 + a_{22}x_2 + \cdots + a_{2s}x_s &= b_2, \\
		\vdots \qquad\quad \vdots \qquad\quad \vdots \qquad\quad \vdots \quad ~&\quad~ \; \vdots \\
		a_{r1}x_1 + a_{r2}x_2 + \cdots + a_{rs}x_s &= b_r.
	\end{align*}
\end{mybox}

\begin{prop}
	Every LP in standard form can be brought into canonical form. In other
	words, every LP has an associated LP in canonical form.
\end{prop}

\WEEK{1}

\begin{proof}
	Since we have already convinced ourselves that every LP can be brought into
	standard form, it suffices to show that we can convert every LP in standard
	form into canonical form. 
	
	The only difference between the two forms are in
	the constraints; namely, we need to convert an inequality of the form 
	\begin{align}\label{eqn:ineq}
		a_1x_1 + \cdots + a_nx_n &\leq b
	\end{align}
	to an equality. To accomplish this, we introduce \emph{slack}
	variables---these are just variables with a pretentious title. They simply
	exist to ``pick up the slack''. The ``slack'' is just the difference of the
	right hand side and the left hand side of~\eqref{eqn:ineq}. 

	Let $s$ be a (slack) variable. Then~\eqref{eqn:ineq} is equivalent to 
	\begin{align*}
		s &\geq 0, \\ 
		a_1x_1 + \cdots + a_nx_n + s &= b.
	\end{align*}
	Hence, we can introduce a new variable for each inequality and obtain an LP
	in canonical form.
\end{proof}

\begin{ex}\label{ex:sewing}
	A tailor is producing jumpers and trousers. They first need to cut the
	fabric and then sew it together. It takes 2 hours to cut the fabric for
	either a pair of trousers or a jumper. It takes 5 hours to sew a pair of
	trousers and 3 hours for a jumper. Scissors can be used for 8 hours per day,
	wheres the sewing machine can be used for 15 hours per day. If a pair of
	trousers is sold for \EUR 120 and a jumper for \EUR 100, how many of each
	should be made to maximize revenue? (Let's ignore demand.)

	Write an LP in canonical form for this scenario.

	\begin{myboxex}[0.65]
		Maximize
		\begin{align*}
			z &= 100J + 120T,
		\end{align*}
		subject to $J,T,s_1,s_2\geq 0$ and
		\begin{align*}
			2J + 2T + s_1 &= 8,\\
			3J + 5T + s_2 &= 15.
		\end{align*}
	\end{myboxex}
\end{ex}

\subsection{Matrix notation}

Instead of writing out all of the constraints and all the terms of the objective
function, we can compactly describe the same data using matrices. Define 
\begin{align*}
	A &= \begin{bmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{m1} & a_{m2} & \cdots & a_{mn}
	\end{bmatrix}, & 
	x &= \begin{bmatrix}
		x_1 \\ x_2 \\ \vdots \\ x_n
	\end{bmatrix}, & 
	b &= \begin{bmatrix}
		b_1 \\ b_2 \\ \vdots \\ b_m 
	\end{bmatrix},  &
	c &= \begin{bmatrix}
		c_1 \\ c_2 \\ \vdots \\ c_n
	\end{bmatrix}. 
\end{align*}

We use the relations $\leq$ and $\geq$ like we do with $=$ when applied to vectors, that is, they are determined coordinate wise. For example 
\begin{align*}
	\begin{bmatrix}
		1 \\ 4
	\end{bmatrix} &\leq 
	\begin{bmatrix}
		2 \\ 5
	\end{bmatrix}, & 
	\begin{bmatrix}
		2 \\ 3 
	\end{bmatrix} \not\leq 
	\begin{bmatrix}
		5 \\ 1 
	\end{bmatrix}.
\end{align*}
In symbols, $x\leq y$ if and only if $x_i\leq y_i$ for all $i$.

\begin{mybox}[LP standard form (matrices)]
	For $A\in\mathrm{Mat}_{m\times n}(\R)$, $b\in\R^m$, and $c\in \R^n$,
	maximize 
	\[
		z = c^{\top}x 
	\]
	subject to $x\geq 0$ and 
	\begin{align*}
		Ax &\leq b.
	\end{align*}
\end{mybox}

\begin{notation}
	The letter $n$ is the number of variables in the objective function, and $m$
	is the number of inequalities separate from $x\geq 0$.
\end{notation}

If it is not already clear how to convert all the previous example above into
the matrix form, try to work this out yourself.

\begin{defn}
	A vector $x\in \mathbb{R}^n$ satisfying all the constraints of an LP (in
	standard form) is a \define{feasible solution}.
\end{defn}

\begin{ex}
	Recall \Cref{ex:sewing} the ``sewing problem''. The following vectors are
	all feasible solutions:
	\begin{align*}
		\begin{bmatrix}
			0 \\ 0 
		\end{bmatrix}, \begin{bmatrix}
			4 \\ 0 
		\end{bmatrix}, \begin{bmatrix}
			0 \\ \pi
		\end{bmatrix}, \begin{bmatrix}
			1 \\ 2 
		\end{bmatrix}.
	\end{align*}
	The following vectors are not feasible solutions:
	\begin{align*}
		\begin{bmatrix}
			-1 \\ 0 
		\end{bmatrix}, \begin{bmatrix}
			5 \\ 0 
		\end{bmatrix}, \begin{bmatrix}
			0 \\ 4
		\end{bmatrix}, \begin{bmatrix}
			2 \\ 2 
		\end{bmatrix}.
	\end{align*}
\end{ex}

\begin{defn}
	A feasible solution that maximizes the objective function of an LP is an
	\define{optimal solution}.
\end{defn}

Now we describe the corresponding matrix form for the canonical form of an LP.
It is built \emph{from} the standard form of an LP. We write
$I_n\in\mathrm{Mat}_{n}(\R)$ for the identity matrix. For matrices $A,B\in
\mathrm{Mat}_{m\times n}(\R)$, we set 
\begin{align*}
	\begin{bmatrix}
		A ~|~ B 
	\end{bmatrix} &= \begin{bmatrix}
		a_{11} & \cdots & a_{1n} & b_{11} & \cdots & b_{1n} \\ 
		\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
		a_{m1} & \cdots & a_{mn} & b_{m1} & \cdots & b_{mn}
	\end{bmatrix} \in \mathrm{Mat}_{m \times 2n}(\R).
\end{align*}

\begin{mybox}[LP canonical form (matrices)]
	For $A\in\mathrm{Mat}_{m\times (n+s)}(\R)$, $b\in\R^m$, and $c\in \R^{n+s}$,
	maximize 
	\begin{align*}
		z &= c^{\top}x,
	\end{align*}
	subject to $x\geq 0$ and 
	\begin{align*}
		Ax &= b,
	\end{align*}
	where $c^{\top} = (c_1,\dots, c_n, 0, \dots, 0)$ and $x^\top=(x_1,\dots,
	x_n,\dots, x_{n+s})$.
\end{mybox}

One can take an LP in standard form and construct one in canonical form with the
(main) constraint equation: 
\begin{align*}
	\begin{bmatrix}
		A ~|~ I_m 
	\end{bmatrix}x &= b.
\end{align*}

\begin{exercise}
	Show that a feasible solution for an LP in standard form induces a feasible
	solution in canonical form. Is the converse true?
\end{exercise}

\subsection{Geometry of the feasibly set}

Now we begin our analysis of the set of feasible solutions to an LP. We begin by
looking at the features of its geometry.

\begin{ex}\label{ex:sewing-feasible}
	Let's consider the standard form of the LP in \Cref{ex:sewing}. In
	particular, the feasible solutions are constrained by $J,T \geq 0$ and 
	\begin{align*}
		2J + 2T &\leq 8, \\
		3J + 5T &\leq 15.
	\end{align*}
	We can plot the region in $\R^2$ as follows:
	\begin{center}
		\begin{tikzpicture}
			\draw[->] (-0.25,0) -- (6,0) node[right] {$J$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$T$};
			\draw (-0.25,4.25) -- (4.25,-0.25);
			\draw (-0.25,3.15) -- (5.416,-0.25);
			\draw[ultra thick, fill=RoyalBlue, opacity=0.8] (0,0) -- (4,0) -- (2.5,1.5) -- (0,3) -- (0,0);
			\node at (1.5, 3.75) {{\footnotesize $J + T = 4$}};
			\node at (5, 1) {{\footnotesize $3J + 5T = 15$}};
		\end{tikzpicture}
	\end{center}
\end{ex}

Let's consider one of our constraint inequalities:
\begin{align*}
	a_{i1}x_1 + \cdots + a_{in}x_n \leq b_i.
\end{align*}
This can be compactly written as $a^{\top}x \leq b_i$ for $a\in\R^n$ and $b_i\in
\R$. The \emph{equation} 
\begin{align*}
	a^{\top}x = b_i 
\end{align*}
defines a \define{hyperplane} in $\R^n$: the vector $a$ describes the ``slope''
and the scalar $b_i$ describes how far the hyperplanes shifts away from the
origin. The hyperplane $a^{\top}x = b_i$ is the boundary of the set of solutions
to $a^{\top}x \leq b_i$. In $\R^2$, hyperplanes are lines, and in $\R^3$ they
are planes. 

\begin{figure}[h]
	\centering 
	\begin{tikzpicture}
		\draw[->] (-2,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-2) -- (0,2) node[above] {$x_2$};
		\draw[<->, ultra thick, RoyalBlue] (-1.9,-0.45) -- (1.9,1.45);
		\draw[<->, thick, Crimson, dashed] (-1.9, -0.95) -- (1.9, 0.95);
	\end{tikzpicture}
	\caption{The line given by $a = (-1, 2)$, $b=1$ in blue, and the line with $a = (-1, 2)$, $b=0$ is in red.}
\end{figure}

Hyperplanes $H$ in $\R^n$ partition $\R^n$ into three sets: the points ``below''
$H$, the points ``above'' $H$, and the points on $H$. The set of points below
$H$ define a \define{half-space}, and similarly for the set of points above $H$. More precisely, if 
\begin{align*}
	H &= \{x\in \R^n \mid a^{\top}x = b\},
\end{align*}
then both 
\begin{align*}
	H^+ &= \{x\in \R^n \mid a^{\top}x > b \}, \\
	H^- &= \{x\in \R^n \mid a^{\top}x < b \}
\end{align*}
are half-spaces of $\R^n$. The \define{closed half-spaces} are 
\begin{align*}
	\overline{H^+} &= \{x\in \R^n \mid a^{\top}x \geq b \} = H^+ \cup H, \\
	\overline{H^-} &= \{x\in \R^n \mid a^{\top}x \leq b \} = H^- \cup H.
\end{align*}

\begin{ex}
	The sets 
	\begin{align*}
		X &= \{x\in \R^4 \mid -x_1 -4x_2 + \sin(1)x_3 \leq \pi \} \\
		Y &= \{y\in \R^4 \mid 7y_1 + 7y_2 + 7y_3 + 7y_4 = 2\}, \\
		Z &= \{z\in\R^4 \mid -8z_1 + 4z_2 - 2z_3 + z_4 > 0\}
	\end{align*}
	respectively define a closed half-space, hyperplane, and half-space in
	$\R^4$.
\end{ex}

What does this have to do with LPs? Our constraint inequalities define closed
half-spaces, and if we want to look at the set of feasible solutions, such
points must satisfy all inequalities. Geometrically, the feasible solutions are
contained in the intersection of all of the close half-spaces, and every point
in this intersection must therefore be a feasible solution since it satisfies
all of the constraints. All of this implies that the set of feasible solutions
is a finite intersection of closed half-spaces.

\begin{ex}
	The following are the constraints for some LP and the corresponding set of feasible solutions.

	\noindent \begin{minipage}{0.35\textwidth}
		\begin{align*}
			x, y, z &\geq 0, \\ 
			5x + 3y + 5z &\leq 15, \\ 
			10x + 4y + 5z &\leq 20.
		\end{align*}
	\end{minipage}~%
	\begin{minipage}{0.6\textwidth}
		\centering
		\begin{tikzpicture}%
			[x={(0.731403cm, -0.141984cm)},
			y={(0.681946cm, 0.152416cm)},
			z={(-0.000094cm, 0.978064cm)},
			scale=1.000000,
			back/.style={loosely dotted, thin},
			edge/.style={color=black, thick},
			facet/.style={fill=RoyalBlue,fill opacity=0.800000},
			vertex/.style={inner sep=1pt,circle,draw=green!25!black,fill=green!75!black,thick}]
		\draw[color=black,thick,->] (0,0,0) -- (3.5,0,0) node[anchor=north east]{$x$};
		\draw[color=black,thick,->] (0,0,0) -- (0,5.5,0) node[anchor=north west]{$y$};
		\draw[color=black,thick,->] (0,0,0) -- (0,0,3.5) node[anchor=south]{$z$};
		\coordinate (0.00000, 5.00000, 0.00000) at (0.00000, 5.00000, 0.00000);
		\coordinate (2.00000, 0.00000, 0.00000) at (2.00000, 0.00000, 0.00000);
		\coordinate (1.00000, 0.00000, 2.00000) at (1.00000, 0.00000, 2.00000);
		\coordinate (0.00000, 0.00000, 3.00000) at (0.00000, 0.00000, 3.00000);
		\coordinate (0.00000, 0.00000, 0.00000) at (0.00000, 0.00000, 0.00000);
		\draw[edge,back] (0.00000, 5.00000, 0.00000) -- (0.00000, 0.00000, 0.00000);
		\fill[facet] (1.00000, 0.00000, 2.00000) -- (0.00000, 5.00000, 0.00000) -- (2.00000, 0.00000, 0.00000) -- cycle {};
		\fill[facet] (0.00000, 0.00000, 3.00000) -- (0.00000, 5.00000, 0.00000) -- (1.00000, 0.00000, 2.00000) -- cycle {};
		\fill[facet] (0.00000, 0.00000, 0.00000) -- (2.00000, 0.00000, 0.00000) -- (1.00000, 0.00000, 2.00000) -- (0.00000, 0.00000, 3.00000) -- cycle {};
		\draw[edge] (0.00000, 5.00000, 0.00000) -- (2.00000, 0.00000, 0.00000);
		\draw[edge] (0.00000, 5.00000, 0.00000) -- (1.00000, 0.00000, 2.00000);
		\draw[edge] (0.00000, 5.00000, 0.00000) -- (0.00000, 0.00000, 3.00000);
		\draw[edge] (2.00000, 0.00000, 0.00000) -- (1.00000, 0.00000, 2.00000);
		\draw[edge] (2.00000, 0.00000, 0.00000) -- (0.00000, 0.00000, 0.00000);
		\draw[edge] (1.00000, 0.00000, 2.00000) -- (0.00000, 0.00000, 3.00000);
		\draw[edge] (0.00000, 0.00000, 3.00000) -- (0.00000, 0.00000, 0.00000);
		\end{tikzpicture}
	\end{minipage}
\end{ex}

But wait, there's more! The objective function in an LP in standard form is
linear: 
\begin{align*}
	z &= c^{\top}x.
\end{align*}
We rephrase the LP in the following way.

\begin{mybox}[Hyperplanes all the way down]
	Find the largest $k\in \R$ such that 
	\begin{align*}
		c^{\top}x = k
	\end{align*}
	subject to $x\geq 0$ and
	\begin{align*}
		Ax &\leq b.
	\end{align*}
\end{mybox}

\begin{ex}
	Let's bring in the objective function from the sewing problem in
	\Cref{ex:sewing}. We want to maximize 
	\begin{align*}
		z &= 100J + 120T.
	\end{align*}
	Instead, let's plot a number of hyperplanes (i.e.\ lines) of the form 
	\begin{align*}
		100J + 120T = k
	\end{align*}
	for different values of $k$. We use the feasibility region plotted in
	\Cref{ex:sewing-feasible}.
	\begin{center}
		\begin{tikzpicture}
			\draw[->] (-0.25,0) -- (6,0) node[right] {$J$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$T$};
			\draw[ultra thick, fill=RoyalBlue, opacity=0.8] (0,0) -- (4,0) -- (2.5,1.5) -- (0,3) -- (0,0);
			\draw[ultra thick, Crimson] (-0.25, 0.21) -- (0.3, -0.25);
			\draw[ultra thick, Crimson] (-0.25, 1.875) -- (2.3, -0.25);
			\draw[ultra thick, Crimson] (-0.25, 3.54) -- (4.3, -0.25);
			\node at (0.3,-0.5) {{\footnotesize $k=0$}};
			\node at (2.4,-0.5) {{\footnotesize $k=200$}};
			\node at (4.4,-0.5) {{\footnotesize $k=400$}};
		\end{tikzpicture}
	\end{center}
	We can see the optimal solution to the sewing problem.
\end{ex}

\begin{ex}\label{ex:no-optimal}
	Consider the following LP.

	\begin{myboxex}[0.65]
		Maximize 
		\begin{align*}
			z &= 2x + 5y
		\end{align*}
		subject to $x,y\geq 0$ and 
		\begin{align*}
			-3x+2y &\leq 6, \\ 
			-x-2y &\leq -2
		\end{align*}
	\end{myboxex}

	Does the LP have an optimal solution? Let's plot the feasible solutions and
	a few hyperplanes of the form $2x+5y = k$. 
	\begin{center}
		\begin{tikzpicture}
			\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
			\fill[fill=RoyalBlue, opacity=0.8] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0) -- (5,5);
			\draw[ultra thick] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0);
			\draw[ultra thick, Crimson] (-0.25, 2.1) -- (5.25, -0.1);
			\draw[ultra thick, Crimson] (-0.25, 4.1) -- (5.25, 1.9);
			\draw[ultra thick, Crimson] (1.875, 5.25) -- (5.25, 3.9);
			\node at (-0.85, 2.1) {{\footnotesize $k=10$}};
			\node at (-0.85, 4.1) {{\footnotesize $k=20$}};
			\node at (1.9, 5.5) {{\footnotesize $k=30$}};
		\end{tikzpicture}
	\end{center}
	Note that the feasible solutions are not \emph{bounded}---more on this
	later. No matter how large a $k$ we get, we can always find feasible
	solutions that yield a larger $k$. Hence, there is no optimal solution.
\end{ex}

\begin{ex}
	We take the LP from \Cref{ex:no-optimal} and change the objective function
	slightly. Does it have an optimal solution?

	\begin{myboxex}[0.65]
		Minimize
		\begin{align*}
			z &= 3x + 5y
		\end{align*}
		subject to $x,y\geq 0$ and 
		\begin{align*}
			-3x+2y &\leq 6, \\ 
			-x-2y &\leq -2
		\end{align*}
	\end{myboxex}

	Again, we'll just plot it.
	\begin{center}
		\begin{tikzpicture}
			\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
			\fill[fill=RoyalBlue, opacity=0.8] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0) -- (5,5);
			\draw[ultra thick] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0);
			\draw[ultra thick, Crimson] (-0.25, 1.15) -- (2.083, -0.25);
			\draw[ultra thick, Crimson] (-0.25, 2.15) -- (3.75, -0.25);
			\draw[ultra thick, Crimson] (-0.25, 4.15) -- (5.25, 0.85);
			\draw[ultra thick, Crimson] (1.25, 5.25) -- (5.25, 2.85);
			\node at (-0.8, 1.1) {{\footnotesize $k=5$}};
			\node at (-0.85, 2.1) {{\footnotesize $k=10$}};
			\node at (-0.85, 4.1) {{\footnotesize $k=20$}};
			\node at (1.2, 5.5) {{\footnotesize $k=30$}};
		\end{tikzpicture}
	\end{center}
	There is an optimal solution. It occurs at $(0,1)$ where $z=5$.
\end{ex}

\begin{defn}\label{def:bounded}
	A set $S\subseteq \R^n$ is \define{bounded} if there exists $r>0$ such that
	for all $u,v\in S$ the Euclidean distance $d(u,v) \leq r$.
\end{defn}

Note the order of quantifiers in \Cref{def:bounded}! Informally speaking, a set
is bounded if we can wrap it in a ball (of finite radius). Are feasible
solutions always bounded? Unbounded? 


\subsection{Convexity}

Let's look at some of the feasible solutions we have plotted so far. 

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.6]
			\draw[->] (-0.25,0) -- (6,0) node[right] {$J$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$T$};
			\draw[ultra thick, fill=RoyalBlue, opacity=0.8] (0,0) -- (4,0) -- (2.5,1.5) -- (0,3) -- (0,0);
		\end{tikzpicture}
	\end{subfigure}%
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}[scale=0.6]
			\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
    		\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
			\fill[fill=RoyalBlue, opacity=0.8] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0) -- (5,5);
			\draw[ultra thick] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0);
		\end{tikzpicture}
	\end{subfigure}%
	\begin{subfigure}{0.3\textwidth}
		\centering
		\begin{tikzpicture}%
			[x={(0.731403cm, -0.141984cm)},
			y={(0.681946cm, 0.152416cm)},
			z={(-0.000094cm, 0.978064cm)},
			scale=1.000000,
			back/.style={loosely dotted, thin},
			edge/.style={color=black, thick},
			facet/.style={fill=RoyalBlue,fill opacity=0.800000},
			vertex/.style={inner sep=1pt,circle,draw=green!25!black,fill=green!75!black,thick}, scale=0.9]
			\draw[color=black,thick,->] (0,0,0) -- (3.5,0,0) node[anchor=north east]{$x$};
			\draw[color=black,thick,->] (0,0,0) -- (0,5.5,0) node[anchor=north west]{$y$};
			\draw[color=black,thick,->] (0,0,0) -- (0,0,3.5) node[anchor=south]{$z$};
			\coordinate (0.00000, 5.00000, 0.00000) at (0.00000, 5.00000, 0.00000);
			\coordinate (2.00000, 0.00000, 0.00000) at (2.00000, 0.00000, 0.00000);
			\coordinate (1.00000, 0.00000, 2.00000) at (1.00000, 0.00000, 2.00000);
			\coordinate (0.00000, 0.00000, 3.00000) at (0.00000, 0.00000, 3.00000);
			\coordinate (0.00000, 0.00000, 0.00000) at (0.00000, 0.00000, 0.00000);
			\draw[edge,back] (0.00000, 5.00000, 0.00000) -- (0.00000, 0.00000, 0.00000);
			\fill[facet] (1.00000, 0.00000, 2.00000) -- (0.00000, 5.00000, 0.00000) -- (2.00000, 0.00000, 0.00000) -- cycle {};
			\fill[facet] (0.00000, 0.00000, 3.00000) -- (0.00000, 5.00000, 0.00000) -- (1.00000, 0.00000, 2.00000) -- cycle {};
			\fill[facet] (0.00000, 0.00000, 0.00000) -- (2.00000, 0.00000, 0.00000) -- (1.00000, 0.00000, 2.00000) -- (0.00000, 0.00000, 3.00000) -- cycle {};
			\draw[edge] (0.00000, 5.00000, 0.00000) -- (2.00000, 0.00000, 0.00000);
			\draw[edge] (0.00000, 5.00000, 0.00000) -- (1.00000, 0.00000, 2.00000);
			\draw[edge] (0.00000, 5.00000, 0.00000) -- (0.00000, 0.00000, 3.00000);
			\draw[edge] (2.00000, 0.00000, 0.00000) -- (1.00000, 0.00000, 2.00000);
			\draw[edge] (2.00000, 0.00000, 0.00000) -- (0.00000, 0.00000, 0.00000);
			\draw[edge] (1.00000, 0.00000, 2.00000) -- (0.00000, 0.00000, 3.00000);
			\draw[edge] (0.00000, 0.00000, 3.00000) -- (0.00000, 0.00000, 0.00000);
		\end{tikzpicture}
	\end{subfigure}
	\caption{Three sets of feasible solutions}
\end{figure}

These regions have the property that if one takes two points in that region, say
$u$ and $v$, then all of the points on the line segment between $u$ and $v$ are
also in that region. This is not true of all sets in $\R^n$; can you draw an
example? 

The formula for the line segment between points $u$ and $v$ is 
\begin{align*}
	L_{u,v}(t) &= vt + (1 - t)u
\end{align*}
where $t\in [0,1]$. Note that at the endpoints, we have 
\begin{align*}
	L_{u,v}(0) &= u, & L_{u,v}(1) &= v.
\end{align*}
In the middle, we have points like $L_{u,v}(1/2) = (u + v)/2$ and $L_{u,v}(1/5)
= (4u + v)/5$. 

\begin{defn}
	A set $S\subseteq \R^n$ is \define{convex} if it contains all points on all
	line segments between every pair of points in $S$. In symbols, this means
	that for all $u,v\in S$, 
	\begin{align*}
		\{L_{u,v}(t) \mid t\in [0,1]\} \subseteq S.
	\end{align*}
\end{defn}

\begin{prop}\label{prop:convex-LP}
	The feasible set of solutions of an LP forms a convex set. 
\end{prop}

\WEEK{2}

Try to prove \Cref{prop:convex-LP} yourself. Consider first proving that closed
half-spaces are convex. 

Let's consider two distinct feasible solutions $u$ and $v$ to an LP. We have two
cases: either they take the same value in the objective function or they have
different values. Assume the first, that is, suppose 
\begin{align*}
	c^{\top}u = c^{\top}v.
\end{align*}
What can we say about the values of the points on the line segment $L_{u,v}$?
Let $w = vt + (1 - t)u$ for some $t\in [0, 1]$. Then 
\begin{align*}
	c^{\top}w &= c^{\top}(tv + (1 - t)u) \\
	&= c^{\top}tv + c^{\top}(1-t)u \\
	&= tc^{\top}v + c^{\top}u - tc^{\top}u \\
	&= c^{\top}u.
\end{align*}
Hence, all points on the $L_{u,v}$ have the same value under the objective
function. 

Let's consider the second case, that is, the values are distinct. Assume that 
\begin{align*}
	c^{\top} u < c^{\top}v,
\end{align*}
and suppose $w = tv + (1-t)u$ for some $t\in [0,1]$. Some of the same analysis applies in this case: namely, 
\begin{align*}
	c^{\top}w &= tc^{\top}v + c^{\top}u - tc^{\top}u \\
	&= c^{\top}u + t(c^{\top}v - c^{\top}u).
\end{align*}
Therefore, for all $t\in [0,1]$, we have 
\begin{align*}
	c^{\top}u \leq c^{\top}w \leq c^{\top}v.
\end{align*}
Hence, the endpoint of $L_{u,v}$ have the extreme values. 

We summarize all of this in the following proposition. 

\begin{prop}
	Let $S$ be the set of feasible solutions to an LP. If $L\subseteq S$ is a
	line segment, then one of the following holds.
	\begin{enumerate}
		\item The objective function is constant on $L$.
		\item The endpoints of $L$ are the extreme points under the objective
		function. 
	\end{enumerate}
\end{prop}

\subsection{Convex polyhedra}

We briefly leave the world of linear optimization and discuss some polyhedral
geometry.

\begin{defn}
	A \define{convex polyhedron} is a finite intersection of closed half-spaces
	in $\R^n$.
\end{defn}

Examples include regular polygons in $\R^2$, infinite cone in $\R^2$, and the
platonic solids in $\R^3$. As we have discussed before, the set of feasible
solutions of an LP are, therefore, convex polyhedra. Some non-examples include
balls in every dimension and any non-convex set.

\begin{defn}
	A point $x\in \R^n$ is a \define{convex combination} of points $t_1,\dots,
	t_r\in \R^n$ if there exist $\lambda_1,\dots, \lambda_r \in [0,1]$, with
	$\lambda_1 + \cdots + \lambda_r = 1$, such that 
	\[ 
		x = \lambda_1t_1 + \cdots + \lambda_rt_r. 
	\]
\end{defn}

\begin{ex}
	The point $(2,1,0)^\top$ is a convex combination of 
	\[ 
		\begin{bmatrix}
			4\\ 4 \\ 2 
		\end{bmatrix}, \begin{bmatrix}
			0 \\ -4 \\ 0
		\end{bmatrix}, \begin{bmatrix}
			0 \\ 0 \\ -4
		\end{bmatrix}. 
	\] 
	Take $(\lambda_1,\lambda_2,\lambda_3) = (1/2, 1/4, 1/4)$.
\end{ex}

The reason for the name ``convex combination'' is that the set of points that
are convex combinations of a set of points is convex.

\begin{defn}
	A point $x$ in a convex set $S\subseteq \R^n$ is \define{extreme} if for
	every line segment in $S$, the point $x$ is not in the interior. 
\end{defn}

\begin{ex}
	In \Cref{fig:2-convex}, one of the convex sets has infinitely many extreme
	points, and the other has exactly three. Which is which?
	\begin{figure}
		\centering
		\begin{subfigure}{0.45\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
				\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
				\fill[fill=RoyalBlue, opacity=0.8] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0) -- (5,5);
				\draw[ultra thick] (1.33,5) -- (0,3) -- (0,1) -- (2,0) -- (5,0);
			\end{tikzpicture}
		\end{subfigure}~%
		\begin{subfigure}{0.45\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
				\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
				\draw[ultra thick, fill=RoyalBlue, opacity=0.8] (0,0) -- (15:5) arc(15:75:5) -- cycle;
			\end{tikzpicture}
		\end{subfigure}
		\caption{Two convex sets}
		\label{fig:2-convex}
	\end{figure}
\end{ex}

\begin{prop}\label{prop:extreme-convex}
	Let $S\subseteq \R^n$ be convex. A point $x\in S$ is extreme if and only if
	$x$ is not a convex combination of other points in $S$.
\end{prop}

We won't prove \Cref{prop:extreme-convex}.

\subsection{Extreme point theorem}
\label{sec:extreme}

We state and prove some of the fundamental theorems in the theory of linear optimization. 

\begin{thm}[Extreme points of an LP]
	Let $S$ be the set of feasible solutions to an LP.
	\begin{enumerate}[label=$(\arabic*)$]
		\item If $S$ is non-empty and bounded, then an optimal solution exists
		and occurs as an extreme point of $S$.
		\item If $S$ is non-empty, unbounded, and contains an optimal solution,
		then the optimal solution occurs as an extreme point of $S$.
		\item If an optimal solution does not exist, then either $S$ is empty or
		unbounded.
	\end{enumerate}
\end{thm}

\begin{proof}[Proof sketch]
	A subset of $\mathbb{R}^n$ is compact if and only if it is closed and
	bounded (Heine--Borel Theorem). Continuous real-valued functions on compact
	sets have a global maximum (fact from metric spaces or topology). The seat
	of feasible solutions form a convex polyhedron. By
	\Cref{prop:extreme-convex}, an optimal solution is an extreme point.
\end{proof}

\begin{ex}\label{ex:extreme-pts-dim2}
	Show that the following LP has infinitely many optimal solutions for $f(x,y)
	= 4x + 4y$. Show that for $f(x,y) = 4x + y$ there is a unique optimal
	solution.

	\begin{myboxex}[0.7]
		Maximize:
		\begin{align*}
			z &= f(x,y),
		\end{align*}
		subject to the constraints: $x,y\geq 0$ and 
		\begin{align*}
			-2x -y &\leq -2, \\
			x - y &\leq 2, \\
			x + y &\leq 3.
		\end{align*}
	\end{myboxex}

	The plot of this LP looks like the following.
	\begin{center}
		\begin{tikzpicture}
			\draw[->] (-0.25,0) -- (5,0) node[right] {$x$};
			\draw[->] (0,-0.25) -- (0,5) node[above] {$y$};
			\draw[ultra thick, fill=RoyalBlue, opacity=0.8] (0,2*1.2) -- (0,3*1.2) -- (2.5*1.2,0.5*1.2) -- (2*1.2,0) -- (1*1.2,0) -- cycle;
		\end{tikzpicture}
	\end{center}
	The extreme points are given by 
	\begin{align*}
		\{(1,0),\ (2,0),\ (5/2, 1/2),\ (0, 2),\ (0, 3)\}.
	\end{align*}
	With $f(x,y)=4x+4y$, the values are $4, 8, 12, 8, 12$, respectively.
	Therefore, all points on the line segment between $(0,3)$ and $(5/2, 1/2)$
	are optimal solutions. If, instead, $f(x,y)=4x+y$, then the values are
	$4,8,10.5, 2, 3$. Hence, we have a unique optimal solution at $(5/2, 1/2)$.

	Note that we had infinitely many optimal solutions when the line determined
	by $f(x,y)=0$ was parallel to one of our constraints---the converse is not
	true in general: try $f(x,y)=4x+2y$, which is parallel to the first
	constraint.
\end{ex}

\Cref{ex:extreme-pts-dim2} is simple because we can visualize fairly easily all
of the extreme points. In fact, all extreme points for an LP with two variables
occur at intersections of lines, which are easy to handle. Without some
additional tools, it is not an easy task to find extreme points of
higher-dimensional LPs. The next two theorems provide a roadmap to find these
extreme points. 


\begin{thm}\label{thm:extreme-I}
	Suppose we have an LP in canonical form with constraints $x\geq 0$ and
	$Ax=b$ for some $A=\begin{bmatrix} a_1 & a_2 & \cdots & a_{n+s}
	\end{bmatrix}\in\mathrm{Mat}_{m\times n+s}(\R)$. Assume that 
	\begin{itemize}
		\item the first $m$ columns of $A$, i.e.\ $\{a_1, \dots, a_m\}$, are linearly independent, and
		\item for some $x_1', \dots, x_m'\geq 0$, we have $x_1'a_1 + \cdots + x_m'a_m = b$.
	\end{itemize}
	Then the following is an extreme point of the set of feasible solutions: 
	\begin{align*}
		& (x_1', \dots, x_m', 0, \dots, 0).
	\end{align*}
\end{thm}

\begin{proof}
	By assumptions, we know that $x = (x_1', \dots, x_m', 0\dots, 0)$ is
	feasible. We need to show it is extreme. Assume $x$ is not extreme, so there
	exists feasible points $u,v\in\mathbb{R}^m$ and $t\in (0, 1)$ such that 
	\begin{align*}
		x &= tv + (1-t)u.
	\end{align*}
	This implies that for all $i\in {m+1,\dots, n+s}$ and all $j\in \{1,\dots, m\}$ we have 
	\begin{align*}
		tv_i + (1-t)u_i &= 0, \\
		tv_j + (1-t)u_j &= x_j'.
	\end{align*}
	Since $t\in (0,1)$
	and $u_i,v_i\geq 0$, it follows that $u_i=0=v_i$ for $i\in \{m+1,\dots,
	n+s\}$. As $u$ is feasible, $Au=b$. Since $u_{m+1}=\cdots =u_{n+s}=0$, we have 
	\begin{align*}
		u_1a_1 + \cdots + u_ma_m = b.
	\end{align*}
	By our assumptions, $u_j = x_j'$ for all $j\in \{1,\dots, m\}$, which is a contradiction. Hence, $x$ is extreme.
\end{proof}

\begin{thm}\label{thm:extreme-II}
	Suppose we have an LP in canonical form. If $x$ is an extreme point of the
	set of feasible solutions, then the columns of $A$ corresponding to positive
	coordinates of $x$ form a set of linearly independent vectors of
	$\mathbb{R}^m$.
\end{thm}

\WEEK{3}

\begin{proof}[Proof]
	Reorganize the variables so that the first $k$ coordinates of $x$ are positive and all others zero. Thus, 
	\begin{align*}	
		x_1'a_1 + \cdots + x_k'a_k &= b.
	\end{align*}
	Suppose that $\{a_1,\dots, a_k\}$ is linearly dependent, so there exist scalars such that 
	\begin{align*}
		\lambda_1a_1 + \cdots + \lambda_ka_k &= 0,
	\end{align*}
	where not all $\lambda_1, \dots, \lambda_k$ are zero. Therefore, we have two feasible solutions:
	\begin{align*}
		u &= (x_1' - \lambda_1, \dots, x_k' - \lambda_k, 0 \dots, 0), & v &= (x_1' + \lambda_1, \dots, x_k' + \lambda_k, 0 \dots, 0).
	\end{align*}
	Moreover $x = L_{u,v}(1/2)$, contradicting the fact that it is extreme.
	Hence, the set $\{a_1,\dots, a_k\}$ is linearly independent. 
\end{proof}

So from \Cref{thm:extreme-II}, the columns of $A$ corresponding to positive
entries of an extreme point $x$ (contained in the set of feasible solutions to
an LP in canonical form) are linearly independent. Since they exist in $\R^m$,
and we cannot have more than $m$ linearly independent vectors in $\R^m$, we have
the following corollary to \Cref{thm:extreme-II}.

\begin{cor}
	At most $m$ entries of an extreme point can be positive. The rest are zero.
\end{cor}

Given an LP in canonical form with constraint matrix $A\in \mathrm{Mat}_{m\times
s}(\R)$ where $s\geq m$, we can select subsets of $m$ columns of $A$ that are
linearly independent to find extreme points. Let's first give a name to these points.

\begin{defn}
	A \define{basic solution} to $Ax=b$ is a vector $x$ with exactly $m$ nonzero
	entries. The variables associated to the zero entries of $x$ are called
	\define{non-basic variables}, and the others are called \define{basic
	variables}.
\end{defn}

\begin{ex}
	A basic solution to the equation 
	\begin{align*}
		\begin{bmatrix}
			1 & 0 & 1 & 0 & 1 & 0 \\ 
			0 & -1 & -1 & 0 & -1 & -1 \\
			1 & 2 & 2 & 1 & 1 & 1 
		\end{bmatrix} x &= \begin{bmatrix}
			b_1 \\ b_2 \\ b_3
		\end{bmatrix}
	\end{align*}
	is obtained by finding three linearly independent columns. For example, the
	first, fourth, and fifth columns:
	\begin{align*}
		\begin{bmatrix}
			1 & 0 & 1 \\ 
			0 & 0 & -1 \\
			1 & 1 & 1 
		\end{bmatrix} x' &= \begin{bmatrix}
			b_1 \\ b_2 \\ b_3
		\end{bmatrix}
	\end{align*}
	yield the basic solution $x=(b_1+b_2, 0, 0, b_3-b_1, -b_2, 0)$.
\end{ex}

Note that basic solutions need not be feasible solutions, that is, they may
contain negative entries. A feasible solution that is also a basic solution is
called a \define{basic feasible solution}.

\begin{exercise}
	Consider an LP in canonical form with 
	\begin{align*}
		A &= \begin{bmatrix}
			2 & 3 & 1 & 0 & 0 \\ 
			-1 & 1 & 0 & 2 & 1 \\
			0 & 6 & 1 & 0 & 3 
		\end{bmatrix}, & b &= \begin{bmatrix}
			1 \\ 1 \\ 4
		\end{bmatrix}.
	\end{align*}
	Which of the following points are basic solutions?
	\begin{align*}
		u &= \begin{bmatrix}
			0 \\ 2 \\ -5 \\ 0 \\ -1 
		\end{bmatrix}, & v &= \begin{bmatrix}
			0 \\ 0 \\ 1 \\0 \\ 1
		\end{bmatrix}, & w &= \begin{bmatrix}
			1 \\ 0 \\ -1 \\ 1 \\ 0
		\end{bmatrix}
	\end{align*}
	Are any of them basic feasible solutions?
\end{exercise}

\begin{thm}[Extremely basic feasible solutions]
	Every basic feasible solution of an LP in canonical form is an extreme point
	of the set of feasible solutions. The converse is also true.
\end{thm}

Assuming the constraint matrix $A\in\mathrm{Mat}_{m\times s}(\R)$, with $s\geq
m$, then we know an \emph{upper bound} on the number of basic feasible
solutions. It is 
\begin{align*}
	\binom{s}{m} &= \dfrac{s!}{m!(s-m)!} .
\end{align*}

We have been dealing primarily with canonical form. What can we do about standard form?

Suppose $x'\in \R^s$ is an extreme point of the set of feasible solutions in
canonical form. Then by \emph{truncating} $x'$ to $x\in \R^m$ we obtain an
extreme point of the set of feasible solutions in standard form. Thus, we go
from SF to CF via adding slack variables, and from CF to SF by truncating those
slack variables. 

Try these problems out yourself.

\begin{exercise}
	Consider an LP in CF with 
	\begin{align*}
		A &= 
		\begin{bmatrix}
			3 & 0 & 1 & 1 & 0 \\
			2 & 1 & 0 & 0 & 0 \\
			4 & 0 & 3 & 0 & 1 
		\end{bmatrix}, & 
		b &= \begin{bmatrix}
			5 \\ 3\\ 6
		\end{bmatrix}.
	\end{align*}
	Which of the points 
	\begin{align*}
		x_1 &= \begin{bmatrix}
			0 \\ 3 \\ 0 \\ 5 \\ 6 
		\end{bmatrix}, & x_2 &= \begin{bmatrix}
			0 \\ 3 \\ 5 \\ 0 \\ -9 
		\end{bmatrix}, & x_3 &= \begin{bmatrix}
			1 \\ 1 \\ 1/2 \\ 3/2 \\ 1/2
		\end{bmatrix}, & x_4 &= \begin{bmatrix}
			1/2 \\ 1 \\ 1 \\ 0 \\ 2
		\end{bmatrix}, & x_5 &= \begin{bmatrix}
			3/2 \\ 0 \\ 0 \\ 1/2 \\ 0 
		\end{bmatrix}
	\end{align*}
	is 
	\begin{enumerate}[label=$(\roman*)$]
		\item a basic solution,
		\item a basic feasible solution,
		\item an extreme point of the set of feasible solutions,
		\item a feasible solution.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Consider the following LP.

	\begin{myboxex}[0.75]
		Maximize 
		\begin{align*}
			z &= 4x + 2y + 7z
		\end{align*}
		subject to the constraints $x,y,z\geq 0$ and 
		\begin{align*}
			2x - y + 4z &\leq 18, \\
			4x + 2x + 5z &\leq 10.
		\end{align*}
	\end{myboxex}
	\begin{enumerate}
		\item Put this into canonical form. 
		\item For each extreme point of the LP in canonical form, identify the
		basic variables.
		\item Write down all of the extreme points for both the standard form
		and canonical form.
		\item Which of the extreme points are optimal solutions?
	\end{enumerate}
\end{exercise}


\section{The simplex method}

By \Cref{sec:extreme}, optimal solutions to LPs are extreme points of a convex
polyhedron. Although only finitely many, running through all of these points can
be expensive. The key result of the simplex method is that we do not need to
consider \emph{all} extreme points. Instead, we start at an extreme point, and
then move to a ``neighboring'' extreme point if it further maximizes our
objective function.

\subsection{Build up}

\begin{defn}
	Two distinct extreme points of an LP in CF are \define{adjacent} if as basic
	feasible solutions they have all but one basic variable in common. 
\end{defn}

\begin{ex}
	The pair of extreme points 
	\[
		(0 , 0 , 8 , 15)^{\top}
		\qquad (3 , 0 , 2 , 0)^{\top}
	\]
	are adjacent, but the following pairs of points are not:
	\[ 
		(0 , 0 , 8 , 15)^{\top}
		\qquad (3/2 , 5/2 , 0 , 0)^{\top}.
	\]
\end{ex}

\WEEK{4}

Let's consider the sewing problem from \Cref{ex:sewing} again. In canonical
form, we have the following LP.

\begin{myboxex}[0.75]
	Maximize
	\begin{align*}
		z &= 100J + 120T
	\end{align*}
	subject to the constraints $J,T,s_1,s_2\geq 0$ and 
	\begin{align*}
		2J + 2T + s_1 \phantom{+ s_2 } &= 8, \\ 
		3J + 5Y \phantom{+ s_1 } + s_2 &= 15.
	\end{align*}
\end{myboxex}

There is an obvious recipe to cook up a basic feasible solution: set all
non-slack variables to $0$ and ``solve'' for the slack variables. In our case,
the point is $(0,0,8,15)^{\top}$, and the basic variables are $s_1$ and $s_2$.
Really, this should be thought of taking the third and fourth columns and
solving.
\begin{align*}
	\begin{bmatrix}
		1 & 0 \\ 
		0 & 1 
	\end{bmatrix}
	\begin{bmatrix}
		s_1 \\ s_2 
	\end{bmatrix} &= \begin{bmatrix}
		8 \\ 15
	\end{bmatrix} .
\end{align*}

We can move to an adjacent extreme point by swapping out a basic variable. We
could replace $s_1$ with $J$ for example. Thus we would take the first and last
columns of $A$ to solve
\begin{align*}
	\begin{bmatrix}
		2 & 0 \\
		3 & 1 
	\end{bmatrix} \begin{bmatrix}
		J \\ s_2 
	\end{bmatrix} &= \begin{bmatrix}
		8 \\ 15
	\end{bmatrix}.
\end{align*}
This yields the solution $(4, 0, 0, 3)^{\top}$. We could have instead exchanged
$s_2$ with $T$ yielding
\begin{align*}
	\begin{bmatrix}
		1 & 2 \\
		0 & 5 
	\end{bmatrix} \begin{bmatrix}
		s_1 \\ T 
	\end{bmatrix} &= \begin{bmatrix}
		8 \\ 15
	\end{bmatrix}
\end{align*}
and a solution of $(0, 3, 2, 0)^{\top}$. For now, these choices are arbitrary,
but this is the game that is played with the simplex method.

We will initialize a tableau to guide us through extreme points.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 2 & 2 & 1 & 0 & 0 & 8 \\ \hline
		$s_2$ & 3 & 5 & 0 & 1 & 0 & 15 \\ \hline 
		& $-100$ & $-120$ & 0 & 0 & 1 & 0 \\ \hline
	\end{tabular}
\end{center}
Here, we have the two basic variables in the first (leftmost) column. The top
row has all of the variables plus the objective function. The second row
corresponds to the first constraint equation: $2J+2T+s_1=8$, and the third row
corresponds to the second constraint equation. The last row is the
\define{objective row}, and it corresponds to $-100J - 120T + z = 0$. The final
column are the values of the equations at the particular extreme point
determined by the basic variables in the first column. For example, the current
extreme point is $(0,0,8,15)$, and evaluating this at $2J+2T+s_1$, at
$3J+5T+s_2$, and $-100J - 120T + z$ yields $8$, $15$, and $0$, respectively.

The Simplex Method can be thought of as a game, where we hop around extreme
points until we are at an optimal solution. Once we hit an optimal solution, we
are done. (We've won!) Otherwise we continue playing. We will address how we can
tell whether or not we are on an optimal solution later. For now, we discuss how
to play.

We move from one basic feasible solution to another adjacent one. We need to
determine a pair of variables to exchange. The \define{entering variable} is the
new basic variable; the \define{departing variable} is the outgoing variable.
The fundamental questions we are to answer right now is how we decide which
variable is which. To determine the entering variable, we apply a simple check:
the entering variable corresponds to the column with the most negative (i.e.
negative and largest in absolute value) coefficient in the objective row. The
departing variable is a bit more complicated and will be more easily seen in an
example.

Now we continue with our example. Note that the variable $T$ has the most
negative coefficient; therefore it is our entering variable. By how much can we
increase $T$? Let's consider $s_1$ and $s_2$ has functions of the other
variables:
\begin{align*}
	s_1 &= 8 - 2J - 2T, \\
	s_2 &= 15 - 3J - 5T. 
\end{align*}
Since we are at $(0,0,8,15)$ and we will have $T$ as the new basic variable, we
have that $J$ is non-basic. Hence, $J=0$, so we can simplify the above
equations:
\begin{align*}
	s_1 &= 8 - 2T, \\
	s_2 &= 15 - 5T. 
\end{align*}
Because $s_1,s_2\geq 0$, we have two implied inequalities:
\begin{equation}\label{eqn:theta-ratios}
	\begin{split}
		4 &\geq T, \\
		3 &\geq T. 	
	\end{split}
\end{equation}
Hence, the most restrictive constraint is that $3\geq T$. In other words, if $T$
were larger than $3$, then $s_2<0$ which is not allowed. However, if $T=3$, then
$s_2=0$ and, hence, is non-basic. Thus, $s_2$ is the departing variable.

The values $4$ and $3$ in \Cref{eqn:theta-ratios} were obtained from $8/2$ and
$15/5$, respectively. They are called \define{$\theta$-ratios}. We decide which
variable should become the departing variable by taking the variable that
corresponds to the smallest positive $\theta$-ratio. Note that one can see these
ratios in the initial tableau by looking at the final column ``divided by'' the
$T$-column. Here is the tableau once again.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 2 & 2 & 1 & 0 & 0 & 8 \\ \hline
		$s_2$ & 3 & 5 & 0 & 1 & 0 & 15 \\ \hline 
		& $-100$ & $-120$ & 0 & 0 & 1 & 0 \\ \hline
	\end{tabular}
\end{center}

The column corresponding to the entering variable is called the \define{entering
column} or the \define{pivotal column}. The row corresponding to the departing
variable is called the \define{departing row} or the \define{pivotal row}. The
entry in both the pivotal row and column is called the \define{pivot}. Hence,
the $5$ is the current pivot.

To form a new table, we update the column of basic variables and apply Gaussian
elimination. We want that the entry in both the $s_1$ row and column to be $1$
and likewise for $T$.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 2 & 2 & 1 & 0 & 0 & 8 \\ \hline
		$T$ & 3/5 & 1 & 0 & 1/5 & 0 & 3 \\ \hline 
		& $-100$ & $-120$ & 0 & 0 & 1 & 0 \\ \hline
	\end{tabular}
\end{center}
Now we apply Gaussian elimination to clear the entries above and below the
pivot to obtain our new tableau:
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 4/5 & 0 & 1 & $-2/5$ & 0 & 2 \\ \hline
		$T$ & 3/5 & 1 & 0 & 1/5 & 0 & 3 \\ \hline 
		& $-28$ & $0$ & 0 & 24 & 1 & 360 \\ \hline
	\end{tabular}
\end{center}
Note we have moved from $(0,0,8,15)$ to $(0,3,2,0)$, and in the standard form of
the same LP, we have moved from $(0,0)$ to $(0,3)$. Because we have negative
entires in the objective row, we continue playing. 

Our new entering variable is $J$ since it corresponds to the most (and only)
negative coefficient in the objective row. The corresponding $\theta$-ratios are 
\begin{align*}
	s_1 &: \dfrac{2}{4/5} = 5/2, \\
	T &: \dfrac{3}{3/5} = 5.
\end{align*}
Since $s_1$ corresponds to the smallest positive ratio, it is the new departing
variable. We update our tableau accordingly.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$J$ & 1 & 0 & 5/4 & $-1/2$ & 0 & 5/2 \\ \hline
		$T$ & 0 & 1 & $-3/4$ & 1/2 & 0 & 3/2 \\ \hline 
		& $0$ & $0$ & 35 & 10 & 1 & 430 \\ \hline
	\end{tabular}
\end{center}
Since there are no negative entries in the objective row, we are done and are
therefore at an optimal solution. The extreme point we are on is $(5/2, 3/2, 0,
0)$, and the corresponding value under the objective function is $430$. To
address a question previously raised: we know when we are at an optimal solution
if none of the entries in the objective row are negative.

I purposefully skipped over two potential issues. Notice that with
$\theta$-ratios, we consider only those that are positive. 
\begin{enumerate}
	\item What do we do with the negative $\theta$-ratios? 
	\item What is we get a divide by zero error? 
\end{enumerate}
We'll consider both of these cases separately and with our running
example---though slightly tweaked to fit these cases.

Suppose we have the following tableau.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 4/5 & 0 & 1 & $-2/5$ & 0 & 2 \\ \hline
		$T$ & $-3/5$ & 1 & 0 & 1/5 & 0 & 3 \\ \hline 
		& $-28$ & $0$ & 0 & 24 & 1 & 360 \\ \hline
	\end{tabular}
\end{center}
Our entering variable is again $J$, and the $\theta$-ratio corresponding to $T$
is negative. Let's consider what this implies about the system. Treating $T$ as
a function of the rest, we have 
\begin{align*}
	T &= 3 - \dfrac{1}{5}s_2 + \dfrac{3}{5}J = 3 + \dfrac{3}{5}J.
\end{align*}
Since $T\geq 0$, the equality together with this inequality imply that 
\begin{align*}
	-5 \leq T,
\end{align*}
but this is not new information since we are already assuming that $T\geq 0$.
Hence, it can be ignored. And this is precisely the conclusion when we encounter
$\theta$-ratios: they do not impose any new constraints on our system and,
therefore, can be ignored.

Similarly, we assume now that we have the following tableau.
\begin{center}
	\begin{tabular}{|c|ccccc|c|}
		\hline
		& $J$ & $T$ & $s_1$ & $s_2$ & $z$ & \\ \hline
		$s_1$ & 4/5 & 0 & 1 & $-2/5$ & 0 & 2 \\ \hline
		$T$ & $0$ & 1 & 0 & 1/5 & 0 & 3 \\ \hline 
		& $-28$ & $0$ & 0 & 24 & 1 & 360 \\ \hline
	\end{tabular}
\end{center}
We cannot form the $\theta$-ratio for $T$ since that would involve dividing by
$0$. Retreating to the equation of $T$ in terms of the rest yields 
\begin{align*}
	T &= 3 - \dfrac{1}{5}s_2  = 3 .
\end{align*}
This again is not new information and can be deduced from the tableau. Hence,
this (lack of a) $\theta$-ratio can be ignored.

We arrive at two conclusions concerning $\theta$-ratios:
\begin{description}
	\item[Conclusion 1:] Only the positive $\theta$-ratios impose new
	constraints. All other can be ignored.
	\item[Conclusion 2:] If in the pivotal column all entries above the
	objective row are non-positive, then the corresponding LP has no (finite)
	optimal solution.
\end{description}

\WEEK{5}






\newpage

\bibliography{bibliography} 
\bibliographystyle{abbrv}

\end{document}